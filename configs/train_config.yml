# YAML configuration file for run_training.py

# the random seed, for reproducibility
seed: 42

# Images shape
input_shape: !!python/tuple [128, 128, 3]
transformers_input_shape: !!python/tuple [224, 224, 3]

# the type of classification we want to implement
# ['disease', 'plant', 'healthy', 'gen_disease']
class_type: 'disease'

# careful not to use a too large batch size, might lead to OOM errors
batch_size: 32
n_epochs: 20
optimizer: 'adam'
learning_rate: 0.01
lr_decay_rate: 0.1
# use 'sparse_categorical_crossentropy' loss forÂ transformers
loss: 'categorical_crossentropy'

# Option to use pipeline for transformers
transformer: False
# Option to use class weights for imbalanced dataset
class_weights: True
# Option to use PolyLoss as loss function
polyloss: False
# Option to compute advanced metrics while training multiple models
eval_during_training: False
# Models to train
models: ["pret_EfficientNetV2B3", "InceptionResNetV2"]
#, "ResNet50V2","InceptionV3","DenseNet201", "EfficientNetV2B3", "pret_ResNet50V2","pret_DenseNet201","pret_EfficientNetV2B3"]
#"LAB_2path_InceptionV3", "LAB_2path_InceptionResNetV2",

# Option to print all warnings related to data processing
verbose: True
# Option to use wandb for logging
wandb: True
# Option to overwrite the content of the output directory
overwrite_output_dir: True

# the path to the directory of the dataset
dataset: 'resources/datasets/cnn/augm_disease_60343_ds_128.h5'
fe_dataset: 'resources/datasets/transformers_ds_224/vit'
# Output directory where the model checkpoints will be written
output_dir: 'experiments/final'  # -> all augmented

mean_arr: [118.94, 124.72, 104.59]
std_arr: [49.35, 42.97, 54.13]
augm_mean_arr: [118.14, 124.61, 104.01]
augm_std_arr: [49.30, 42.62, 54.95]
augmlab_mean_arr: [129.75, 122.14, 138.48]
augmlab_std_arr: [44.66, 12.08, 15.12]
